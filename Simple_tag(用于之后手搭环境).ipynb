{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入相关模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pygame\n",
    "import numpy as np\n",
    "\n",
    "from pettingzoo import AECEnv\n",
    "from pettingzoo.utils import wrappers\n",
    "from pettingzoo.utils.agent_selector import agent_selector\n",
    "from pettingzoo.utils.conversions import parallel_wrapper_fn\n",
    "\n",
    "import gymnasium\n",
    "from gymnasium import spaces\n",
    "from gymnasium.utils import seeding\n",
    "\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建Agent类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntityState:\n",
    "    # 智能体的位置和速度\n",
    "    def __init__(self):\n",
    "        # physical position\n",
    "        self.p_pos = None\n",
    "        # physical velocity\n",
    "        self.p_vel = None\n",
    "\n",
    "\n",
    "class AgentState(\n",
    "    EntityState\n",
    "): \n",
    "    # 智能体之间的交流(以speaker为例，就是发声)\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.c = None\n",
    "\n",
    "\n",
    "class Action:  # action of the agent\n",
    "    def __init__(self):\n",
    "        # physical action\n",
    "        self.u = None\n",
    "        # communication action\n",
    "        self.c = None\n",
    "\n",
    "\n",
    "class Entity: \n",
    "    # 智能体名字、尺寸、是否可以移动、是否可以碰撞、物体目睹、颜色、最大速度、加速度、状态、初始质量\n",
    "    def __init__(self):\n",
    "        # name\n",
    "        self.name = \"\"\n",
    "        # properties:\n",
    "        self.size = 0.050\n",
    "        # entity can move / be pushed\n",
    "        self.movable = False\n",
    "        # entity collides with others\n",
    "        self.collide = True\n",
    "        # material density (affects mass)\n",
    "        self.density = 25.0\n",
    "        # color\n",
    "        self.color = None\n",
    "        # max speed and accel\n",
    "        self.max_speed = None\n",
    "        self.accel = None\n",
    "        # state\n",
    "        self.state = EntityState()\n",
    "        # mass\n",
    "        self.initial_mass = 1.0\n",
    "\n",
    "    @property\n",
    "    # 返回初始质量\n",
    "    def mass(self):\n",
    "        return self.initial_mass\n",
    "\n",
    "\n",
    "class Landmark(Entity):  # properties of landmark entities\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "class Agent(Entity):  # properties of agent entities\n",
    "    # 智能体的可移动性√，禁止交流×，不能观测到世界×，物理噪声None，交流噪声None，控制范围1，动作召回\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # agents are movable by default\n",
    "        self.movable = True\n",
    "        # cannot send communication signals\n",
    "        self.silent = False\n",
    "        # cannot observe the world\n",
    "        self.blind = False\n",
    "        # physical motor noise amount\n",
    "        self.u_noise = None\n",
    "        # communication noise amount\n",
    "        self.c_noise = None\n",
    "        # control range\n",
    "        self.u_range = 1.0\n",
    "        # state\n",
    "        self.state = AgentState()\n",
    "        # action\n",
    "        self.action = Action()\n",
    "        # script behavior to execute\n",
    "        self.action_callback = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建world环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class World:  # multi-agent world\n",
    "    def __init__(self):\n",
    "        # list of agents and entities (can change at execution-time!)\n",
    "        self.agents = []\n",
    "        self.landmarks = []\n",
    "        # communication channel dimensionality\n",
    "        self.dim_c = 0\n",
    "        # position dimensionality\n",
    "        self.dim_p = 2\n",
    "        # color dimensionality\n",
    "        self.dim_color = 3\n",
    "        # simulation timestep\n",
    "        self.dt = 0.1\n",
    "        # physical damping\n",
    "        self.damping = 0.25\n",
    "        # contact response parameters\n",
    "        self.contact_force = 1e2\n",
    "        self.contact_margin = 1e-3\n",
    "\n",
    "    # 获取所有智能体以及landmarks的列表\n",
    "    @property\n",
    "    def entities(self):\n",
    "        return self.agents + self.landmarks\n",
    "\n",
    "    # 策略智能体，不采用callback函数进行动作决策，直接通过策略网络获取动作\n",
    "    @property\n",
    "    def policy_agents(self):\n",
    "        return [agent for agent in self.agents if agent.action_callback is None]\n",
    "\n",
    "    # 脚本智能体，采用callback函数直接获取动作\n",
    "    @property\n",
    "    def scripted_agents(self):\n",
    "        return [agent for agent in self.agents if agent.action_callback is not None]\n",
    "\n",
    "    # world单步迭代\n",
    "    def step(self):\n",
    "        # 采用脚本对智能体进行移动\n",
    "        for agent in self.scripted_agents:\n",
    "            agent.action = agent.action_callback(agent, self)\n",
    "\n",
    "        # 将力进行集成，顺序分别为给动作添加u_noise、碰撞力、集成环境的力\n",
    "        p_force = [None] * len(self.entities)\n",
    "        p_force = self.apply_action_force(p_force)\n",
    "        p_force = self.apply_environment_force(p_force)\n",
    "        self.integrate_state(p_force)\n",
    "                \n",
    "        # 将每个智能体添加上c_noise\n",
    "        for agent in self.agents:\n",
    "            self.update_agent_state(agent)\n",
    "\n",
    "    # 给每个动作添加动作噪声\n",
    "    def apply_action_force(self, p_force):\n",
    "        # set applied forces\n",
    "        for i, agent in enumerate(self.agents):\n",
    "            if agent.movable:\n",
    "                noise = (\n",
    "                    np.random.randn(*agent.action.u.shape) * agent.u_noise\n",
    "                    if agent.u_noise\n",
    "                    else 0.0\n",
    "                )\n",
    "                p_force[i] = agent.action.u + noise\n",
    "        return p_force\n",
    "\n",
    "    # 集成环境碰撞带来的力，用于进一步更新状态\n",
    "    def apply_environment_force(self, p_force):\n",
    "        # simple (but inefficient) collision response\n",
    "        for a, entity_a in enumerate(self.entities):\n",
    "            for b, entity_b in enumerate(self.entities):\n",
    "                if b <= a:\n",
    "                    continue\n",
    "                [f_a, f_b] = self.get_collision_force(entity_a, entity_b)\n",
    "                if f_a is not None:\n",
    "                    if p_force[a] is None:\n",
    "                        p_force[a] = 0.0\n",
    "                    p_force[a] = f_a + p_force[a]\n",
    "                if f_b is not None:\n",
    "                    if p_force[b] is None:\n",
    "                        p_force[b] = 0.0\n",
    "                    p_force[b] = f_b + p_force[b]\n",
    "        return p_force\n",
    "\n",
    "    # 集成状态，用于修改位置、速度\n",
    "    def integrate_state(self, p_force):\n",
    "        for i, entity in enumerate(self.entities):\n",
    "            if not entity.movable:\n",
    "                continue\n",
    "            entity.state.p_pos += entity.state.p_vel * self.dt\n",
    "            entity.state.p_vel = entity.state.p_vel * (1 - self.damping)\n",
    "            if p_force[i] is not None:\n",
    "                entity.state.p_vel += (p_force[i] / entity.mass) * self.dt\n",
    "            if entity.max_speed is not None:\n",
    "                speed = np.sqrt(\n",
    "                    np.square(entity.state.p_vel[0]) + np.square(entity.state.p_vel[1])\n",
    "                )\n",
    "                if speed > entity.max_speed:\n",
    "                    entity.state.p_vel = (\n",
    "                        entity.state.p_vel\n",
    "                        / np.sqrt(\n",
    "                            np.square(entity.state.p_vel[0])\n",
    "                            + np.square(entity.state.p_vel[1])\n",
    "                        )\n",
    "                        * entity.max_speed\n",
    "                    )\n",
    "\n",
    "    # 给动作加上交流噪声\n",
    "    def update_agent_state(self, agent):\n",
    "        if agent.silent:\n",
    "            agent.state.c = np.zeros(self.dim_c)\n",
    "        else:\n",
    "            noise = (\n",
    "                np.random.randn(*agent.action.c.shape) * agent.c_noise\n",
    "                if agent.c_noise\n",
    "                else 0.0\n",
    "            )\n",
    "            agent.state.c = agent.action.c + noise\n",
    "\n",
    "    # 碰撞力的记录，用于之后进行碰撞后的位移\n",
    "    def get_collision_force(self, entity_a, entity_b):\n",
    "        if (not entity_a.collide) or (not entity_b.collide):\n",
    "            return [None, None]  # not a collider\n",
    "        if entity_a is entity_b:\n",
    "            return [None, None]  # don't collide against itself\n",
    "        # compute actual distance between entities\n",
    "        delta_pos = entity_a.state.p_pos - entity_b.state.p_pos\n",
    "        dist = np.sqrt(np.sum(np.square(delta_pos)))\n",
    "        # minimum allowable distance\n",
    "        dist_min = entity_a.size + entity_b.size\n",
    "        # softmax penetration\n",
    "        k = self.contact_margin\n",
    "        penetration = np.logaddexp(0, -(dist - dist_min) / k) * k\n",
    "        force = self.contact_force * delta_pos / dist * penetration\n",
    "        force_a = +force if entity_a.movable else None\n",
    "        force_b = -force if entity_b.movable else None\n",
    "        return [force_a, force_b]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建具体scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scenario():\n",
    "    # 具体作用：配置各类别智能体个数、属性，同时创建障碍物\n",
    "    def make_world(self, num_good=1, num_adversaries=3, num_obstacles=2):\n",
    "        world = World()\n",
    "        # set any world properties first\n",
    "        world.dim_c = 2\n",
    "\n",
    "        # 好的agent数量:num_good\n",
    "        # 敌对agent数量:num_adversaries\n",
    "        # 总的agent数量:num_agents\n",
    "        # 障碍物landmarks数量:num_obstacles\n",
    "        num_good_agents = num_good\n",
    "        num_adversaries = num_adversaries\n",
    "        num_agents = num_adversaries + num_good_agents\n",
    "        num_landmarks = num_obstacles\n",
    "        # add agents\n",
    "        world.agents = [Agent() for i in range(num_agents)]\n",
    "        \n",
    "        # 先创建num_adversaries个敌对agent，然后再创建好的agent，属性上也是从这里定义的\n",
    "        for i, agent in enumerate(world.agents):\n",
    "            agent.adversary = True if i < num_adversaries else False\n",
    "            base_name = \"adversary\" if agent.adversary else \"agent\"\n",
    "            base_index = i if i < num_adversaries else i - num_adversaries\n",
    "            agent.name = f\"{base_name}_{base_index}\"\n",
    "            agent.collide = True\n",
    "            agent.silent = True\n",
    "            agent.size = 0.075 if agent.adversary else 0.05\n",
    "            agent.accel = 3.0 if agent.adversary else 4.0\n",
    "            agent.max_speed = 1.0 if agent.adversary else 1.3\n",
    "\n",
    "        # 创建landmarks模块\n",
    "        # add landmarks\n",
    "        world.landmarks = [Landmark() for i in range(num_landmarks)]\n",
    "        for i, landmark in enumerate(world.landmarks):\n",
    "            landmark.name = \"landmark %d\" % i\n",
    "            landmark.collide = True\n",
    "            landmark.movable = False\n",
    "            landmark.size = 0.2\n",
    "            landmark.boundary = False\n",
    "        return world\n",
    "\n",
    "    # 初始化各个智能体以及障碍物的颜色、状态等\n",
    "    def reset_world(self, world, np_random):\n",
    "        # 将智能体根据其类别设定颜色\n",
    "        for i, agent in enumerate(world.agents):\n",
    "            agent.color = (\n",
    "                np.array([0.35, 0.85, 0.35])\n",
    "                if not agent.adversary\n",
    "                else np.array([0.85, 0.35, 0.35])\n",
    "            )\n",
    "            \n",
    "        # 给障碍物填充颜色\n",
    "        for i, landmark in enumerate(world.landmarks):\n",
    "            landmark.color = np.array([0.25, 0.25, 0.25])\n",
    "            \n",
    "        # 初始化智能体的状态，位置设为-1~1随机分布，维度为2维，速度初始化为0，交流初始化为0.\n",
    "        for agent in world.agents:\n",
    "            agent.state.p_pos = np_random.uniform(-1, +1, world.dim_p)\n",
    "            agent.state.p_vel = np.zeros(world.dim_p)\n",
    "            agent.state.c = np.zeros(world.dim_c)        \n",
    "        \n",
    "        # 初始化障碍物的状态，位置设为--0.9~0.9随机分布，维度为2维，速度初始化为0，交流初始化为0.\n",
    "        for i, landmark in enumerate(world.landmarks):\n",
    "            if not landmark.boundary:\n",
    "                landmark.state.p_pos = np_random.uniform(-0.9, +0.9, world.dim_p)\n",
    "                landmark.state.p_vel = np.zeros(world.dim_p)\n",
    "\n",
    "    # 用来判断智能体是否为敌对智能体，如果是的话返回其与好的智能体碰撞的个数，如果不是返回0.\n",
    "    def benchmark_data(self, agent, world):\n",
    "        # returns data for benchmarking purposes\n",
    "        if agent.adversary:\n",
    "            collisions = 0\n",
    "            for a in self.good_agents(world):\n",
    "                if self.is_collision(a, agent):\n",
    "                    collisions += 1\n",
    "            return collisions\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    # 判断两个智能体是否碰撞，碰撞判断的方法是用两者的距离与二者半径之和作比较，如果距离小于半径之和返回True，否则返回False\n",
    "    def is_collision(self, agent1, agent2):\n",
    "        delta_pos = agent1.state.p_pos - agent2.state.p_pos\n",
    "        dist = np.sqrt(np.sum(np.square(delta_pos)))\n",
    "        dist_min = agent1.size + agent2.size\n",
    "        return True if dist < dist_min else False\n",
    "\n",
    "    # return all agents that are not adversaries\n",
    "    # 返回好的智能体的列表\n",
    "    def good_agents(self, world):\n",
    "        return [agent for agent in world.agents if not agent.adversary]\n",
    "\n",
    "    # return all adversarial agents\n",
    "    # 返回敌对智能体的列表\n",
    "    def adversaries(self, world):\n",
    "        return [agent for agent in world.agents if agent.adversary]\n",
    "\n",
    "    # 分别返回各个智能体的回报值\n",
    "    def reward(self, agent, world):\n",
    "        # Agents are rewarded based on minimum agent distance to each landmark\n",
    "        main_reward = (\n",
    "            self.adversary_reward(agent, world)\n",
    "            if agent.adversary\n",
    "            else self.agent_reward(agent, world)\n",
    "        )\n",
    "        return main_reward\n",
    "\n",
    "    # 计算好的agent的reward\n",
    "    def agent_reward(self, agent, world):\n",
    "        # Agents are negatively rewarded if caught by adversaries\n",
    "        rew = 0\n",
    "        shape = False\n",
    "        adversaries = self.adversaries(world)\n",
    "        if (\n",
    "            shape\n",
    "        ):  # reward can optionally be shaped (increased reward for increased distance from adversary)\n",
    "            # 另一种形式的reward，采用的是增加reward的方式来进行相应的求值，与adv距离的欧氏距离*0.1作为reward的增加值。\n",
    "            for adv in adversaries:\n",
    "                rew += 0.1 * np.sqrt(\n",
    "                    np.sum(np.square(agent.state.p_pos - adv.state.p_pos))\n",
    "                )\n",
    "        \n",
    "        # 采用的是降低reward的方式来进行agent的reward获取，如果与adv碰撞了，那么回报值-10。\n",
    "        if agent.collide:\n",
    "            for a in adversaries:\n",
    "                if self.is_collision(a, agent):\n",
    "                    rew -= 10\n",
    "\n",
    "        # agents are penalized for exiting the screen, so that they can be caught by the adversaries\n",
    "        # 超出边界的处理方法：具体分析之后会将图片传入wiki\n",
    "        def bound(x):\n",
    "            if x < 0.9:\n",
    "                return 0\n",
    "            if x < 1.0:\n",
    "                return (x - 0.9) * 10\n",
    "            return min(np.exp(2 * x - 2), 10)\n",
    "\n",
    "        for p in range(world.dim_p):\n",
    "            x = abs(agent.state.p_pos[p])\n",
    "            rew -= bound(x)\n",
    "\n",
    "        return rew\n",
    "\n",
    "    # 敌对agent的奖励值\n",
    "    # 计算的是所有的敌对agent的奖励值之和，具体的奖励值与好的agent是刚好相反的\n",
    "    # 这里传入agent参数的目的是确定当前任务是需要考虑碰撞的因素的，如果agent.collide=False就不把这个作为reward\n",
    "    def adversary_reward(self, agent, world):\n",
    "        # Adversaries are rewarded for collisions with agents\n",
    "        rew = 0\n",
    "        shape = False\n",
    "        agents = self.good_agents(world)\n",
    "        adversaries = self.adversaries(world)\n",
    "        if (\n",
    "            shape\n",
    "        ):  # reward can optionally be shaped (decreased reward for increased distance from agents)\n",
    "            for adv in adversaries:\n",
    "                rew -= 0.1 * min(\n",
    "                    np.sqrt(np.sum(np.square(a.state.p_pos - adv.state.p_pos)))\n",
    "                    for a in agents\n",
    "                )\n",
    "        if agent.collide:\n",
    "            for ag in agents:\n",
    "                for adv in adversaries:\n",
    "                    if self.is_collision(ag, adv):\n",
    "                        rew += 10\n",
    "        return rew\n",
    "\n",
    "    # 总的观测空间\n",
    "    def observation(self, agent, world):\n",
    "        # get positions of all entities in this agent's reference frame\n",
    "        entity_pos = []\n",
    "\n",
    "        # 获取智能体agent与每个障碍物的距离\n",
    "        for entity in world.landmarks:\n",
    "            if not entity.boundary:\n",
    "                entity_pos.append(entity.state.p_pos - agent.state.p_pos)\n",
    "\n",
    "        # communication of all other agents\n",
    "        comm = []\n",
    "        other_pos = []\n",
    "        other_vel = []\n",
    "\n",
    "        # 将其余agent与当前gaent之间的交流、距离元组、非敌对agent的速度分别保存到三个列表中\n",
    "        for other in world.agents:\n",
    "            if other is agent:\n",
    "                continue\n",
    "            comm.append(other.state.c)\n",
    "            other_pos.append(other.state.p_pos - agent.state.p_pos)\n",
    "            if not other.adversary:\n",
    "                other_vel.append(other.state.p_vel)\n",
    "\n",
    "        # 将当前agent的速度、位置 与 智能体agent与每个障碍物的距离 与 当前agent与其余各个agent(包括敌对agent)的距离元组 与 非敌对agent的速度\n",
    "        # 放置到同样的一个ndarray中\n",
    "        return np.concatenate(\n",
    "            [agent.state.p_vel]\n",
    "            + [agent.state.p_pos]\n",
    "            + entity_pos\n",
    "            + other_pos\n",
    "            + other_vel\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建simpleEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleEnv(AECEnv):\n",
    "    metadata = {\n",
    "        \"render_modes\": [\"human\", \"rgb_array\"],\n",
    "        \"is_parallelizable\": True,\n",
    "        \"render_fps\": 10,\n",
    "    }\n",
    "    \n",
    "    '''\n",
    "    需要传入的参数：\n",
    "        @param: 具体的情景——scenario\n",
    "        @param: 世界环境——world\n",
    "        @param: 每轮最大迭代次数——max_cycles\n",
    "        @param: 表现方式——render_mode\n",
    "        @param: 是否采用连续动作——continuous_actions\n",
    "        @param: 本地比例,即每隔智能体的reward中自己产生的reward的比例,最后会与全局reward进行一次加权求和:      \n",
    "            local_ratio*local_reward+(1-local_ratio)*global_reward\n",
    "        ——local_ratio\n",
    "\n",
    "    类内变量解析：\n",
    "        self.render_mode:表述模式,对应[\"human\", \"rgb_array\"],具体区别就是,前者是直接进行图像展示,后者是保存一个rgb图像array,需要自己采用plt展示\n",
    "        self.viewer:暂不清楚\n",
    "        self.width:render的图像宽度\n",
    "        self.height:render的图像高度\n",
    "        self.screen:基本的surface界面\n",
    "        self.max_size:最大的尺寸\n",
    "        self.game_font:render时界面所采取的字体\n",
    "        self.renderOn:是否开启render\n",
    "        self._seed():设定种子\n",
    "\n",
    "        self.max_cycles:单次render最大步长数\n",
    "        self.scenario:情景设置\n",
    "        self.world:世界设置\n",
    "        self.continuous_actions:是否采用连续空间\n",
    "        self.local_ratio:本地比例\n",
    "\n",
    "        self.agents:所有智能体的名字\n",
    "        self.possible_agents:所有可能的智能体的名字\n",
    "        self._index_map:智能体名字对应的下标的词典  i.e. 'agent_1':0 'agent_2':1\n",
    "        self._agent_selector = agent_selector(self.agents)\n",
    "        self.action_spaces:智能体的动作空间词典,具体用法self.action_spaces[agent.name]            \n",
    "        self.observation_spaces:智能体的观测空间词典,具体用法self.observation_spaces[agent.name]\n",
    "        self.state_space:状态空间,包含了所有智能体的观测维度\n",
    "        self.steps:当前轮步长的序号\n",
    "        self.current_actions:每个智能体当前所采取的动作\n",
    "\n",
    "        self.np_random:随机数调用函数，可通过类似于self.np_random.random()的方式来调用\n",
    "\n",
    "    '''\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        scenario,\n",
    "        world,\n",
    "        max_cycles,\n",
    "        render_mode=None,\n",
    "        continuous_actions=False,\n",
    "        local_ratio=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.render_mode = render_mode\n",
    "        pygame.init()\n",
    "        self.viewer = None\n",
    "        self.width = 700\n",
    "        self.height = 700\n",
    "        self.screen = pygame.Surface([self.width, self.height])\n",
    "        self.max_size = 1\n",
    "        self.game_font = pygame.freetype.Font(\"./secrcode.ttf\", 24)\n",
    "\n",
    "        # Set up the drawing window\n",
    "\n",
    "        self.renderOn = False\n",
    "        self._seed()\n",
    "\n",
    "        self.max_cycles = max_cycles\n",
    "        self.scenario = scenario\n",
    "        self.world = world\n",
    "        self.continuous_actions = continuous_actions\n",
    "        self.local_ratio = local_ratio\n",
    "\n",
    "        self.scenario.reset_world(self.world, self.np_random)\n",
    "\n",
    "        self.agents = [agent.name for agent in self.world.agents]\n",
    "        self.possible_agents = self.agents[:]\n",
    "        self._index_map = {\n",
    "            agent.name: idx for idx, agent in enumerate(self.world.agents)\n",
    "        }\n",
    "\n",
    "        self._agent_selector = agent_selector(self.agents)\n",
    "\n",
    "        # set spaces\n",
    "        self.action_spaces = dict()\n",
    "        self.observation_spaces = dict()\n",
    "        state_dim = 0\n",
    "        for agent in self.world.agents:\n",
    "            if agent.movable:\n",
    "                space_dim = self.world.dim_p * 2 + 1\n",
    "            elif self.continuous_actions:\n",
    "                space_dim = 0\n",
    "            else:\n",
    "                space_dim = 1\n",
    "            if not agent.silent:\n",
    "                if self.continuous_actions:\n",
    "                    space_dim += self.world.dim_c\n",
    "                else:\n",
    "                    space_dim *= self.world.dim_c\n",
    "\n",
    "            obs_dim = len(self.scenario.observation(agent, self.world))\n",
    "            state_dim += obs_dim\n",
    "            if self.continuous_actions:\n",
    "                self.action_spaces[agent.name] = spaces.Box(\n",
    "                    low=0, high=1, shape=(space_dim,)\n",
    "                )\n",
    "            else:\n",
    "                self.action_spaces[agent.name] = spaces.Discrete(space_dim)\n",
    "            self.observation_spaces[agent.name] = spaces.Box(\n",
    "                low=-np.float32(np.inf),\n",
    "                high=+np.float32(np.inf),\n",
    "                shape=(obs_dim,),\n",
    "                dtype=np.float32,\n",
    "            )\n",
    "\n",
    "        self.state_space = spaces.Box(\n",
    "            low=-np.float32(np.inf),\n",
    "            high=+np.float32(np.inf),\n",
    "            shape=(state_dim,),\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "\n",
    "        self.steps = 0\n",
    "\n",
    "        self.current_actions = [None] * self.num_agents\n",
    "\n",
    "    # 获取每个智能体的观测空间\n",
    "    def observation_space(self, agent):\n",
    "        return self.observation_spaces[agent]\n",
    "\n",
    "    # 获取每个智能体的动作空间\n",
    "    def action_space(self, agent):\n",
    "        return self.action_spaces[agent]\n",
    "\n",
    "    # 获取随机数生成器self.np_random以及随机数种子seed\n",
    "    def _seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "\n",
    "    # 获取指定智能体在scenario下的具体观测值\n",
    "    def observe(self, agent):\n",
    "        return self.scenario.observation(\n",
    "            self.world.agents[self._index_map[agent]], self.world\n",
    "        ).astype(np.float32)\n",
    "\n",
    "    # 获取所有可能的智能体在环境下的观测值元组\n",
    "    def state(self):\n",
    "        states = tuple(\n",
    "            self.scenario.observation(\n",
    "                self.world.agents[self._index_map[agent]], self.world\n",
    "            ).astype(np.float32)\n",
    "            for agent in self.possible_agents\n",
    "        )\n",
    "        return np.concatenate(states, axis=None)\n",
    "\n",
    "    # 将奖励函数值以及各种变量进行初始化\n",
    "    def reset(self, seed=None, options=None):\n",
    "        if seed is not None:\n",
    "            self._seed(seed=seed)\n",
    "        self.scenario.reset_world(self.world, self.np_random)\n",
    "\n",
    "        self.agents = self.possible_agents[:]\n",
    "        self.rewards = {name: 0.0 for name in self.agents}\n",
    "        self._cumulative_rewards = {name: 0.0 for name in self.agents}\n",
    "        self.terminations = {name: False for name in self.agents}\n",
    "        self.truncations = {name: False for name in self.agents}\n",
    "        self.infos = {name: {} for name in self.agents}\n",
    "\n",
    "        self.agent_selection = self._agent_selector.reset()\n",
    "        self.steps = 0\n",
    "\n",
    "        self.current_actions = [None] * self.num_agents\n",
    "\n",
    "    # 先将动作设定到指定的各个agent上，进行一次世界移动后计算全局reward，并对每个智能体求一个全局+局部的reward\n",
    "    def _execute_world_step(self):\n",
    "        for i, agent in enumerate(self.world.agents):\n",
    "            action = self.current_actions[i]\n",
    "            scenario_action = []\n",
    "            if agent.movable:\n",
    "                mdim = self.world.dim_p * 2 + 1\n",
    "                if self.continuous_actions:\n",
    "                    scenario_action.append(action[0:mdim])\n",
    "                    action = action[mdim:]\n",
    "                else:\n",
    "                    scenario_action.append(action % mdim)\n",
    "                    action //= mdim\n",
    "            if not agent.silent:\n",
    "                scenario_action.append(action)\n",
    "            self._set_action(scenario_action, agent, self.action_spaces[agent.name])\n",
    "\n",
    "        self.world.step()\n",
    "\n",
    "        global_reward = 0.0\n",
    "        if self.local_ratio is not None:\n",
    "            global_reward = float(self.scenario.global_reward(self.world))\n",
    "\n",
    "        for agent in self.world.agents:\n",
    "            agent_reward = float(self.scenario.reward(agent, self.world))\n",
    "            if self.local_ratio is not None:\n",
    "                reward = (\n",
    "                    global_reward * (1 - self.local_ratio)\n",
    "                    + agent_reward * self.local_ratio\n",
    "                )\n",
    "            else:\n",
    "                reward = agent_reward\n",
    "\n",
    "            self.rewards[agent.name] = reward\n",
    "\n",
    "    # 为指定agent设定动作，修改agent对应的action，通过修改agent实例类方法实现\n",
    "    def _set_action(self, action, agent, action_space, time=None):\n",
    "        agent.action.u = np.zeros(self.world.dim_p)\n",
    "        agent.action.c = np.zeros(self.world.dim_c)\n",
    "\n",
    "        if agent.movable:\n",
    "            # 设定加速度，加速度的范围值根据agent.accel设定，如果参数没有定义的话那么默认为5.\n",
    "            agent.action.u = np.zeros(self.world.dim_p)\n",
    "            if self.continuous_actions:\n",
    "                # Process continuous action as in OpenAI MPE\n",
    "                agent.action.u[0] += action[0][1] - action[0][2]\n",
    "                agent.action.u[1] += action[0][3] - action[0][4]\n",
    "            else:\n",
    "                if action[0] == 1:\n",
    "                    agent.action.u[0] = -1.0\n",
    "                if action[0] == 2:\n",
    "                    agent.action.u[0] = +1.0\n",
    "                if action[0] == 3:\n",
    "                    agent.action.u[1] = -1.0\n",
    "                if action[0] == 4:\n",
    "                    agent.action.u[1] = +1.0\n",
    "            sensitivity = 5.0\n",
    "            if agent.accel is not None:\n",
    "                sensitivity = agent.accel\n",
    "            agent.action.u *= sensitivity\n",
    "            action = action[1:]\n",
    "\n",
    "        # 设定交流动作\n",
    "        if not agent.silent:\n",
    "            if self.continuous_actions:\n",
    "                agent.action.c = action[0]\n",
    "            else:\n",
    "                agent.action.c = np.zeros(self.world.dim_c)\n",
    "                agent.action.c[action[0]] = 1.0\n",
    "            action = action[1:]\n",
    "        assert len(action) == 0\n",
    "\n",
    "    # 进行单步迭代\n",
    "    def step(self, action):\n",
    "        if (\n",
    "            self.terminations[self.agent_selection]\n",
    "            or self.truncations[self.agent_selection]\n",
    "        ):\n",
    "            self._was_dead_step(action)\n",
    "            return\n",
    "        \n",
    "        # 参数的类型为str，表示智能体的ID\n",
    "        # 循环对智能体进行状态迭代\n",
    "        cur_agent = self.agent_selection\n",
    "        current_idx = self._index_map[self.agent_selection]\n",
    "        next_idx = (current_idx + 1) % self.num_agents\n",
    "        self.agent_selection = self._agent_selector.next()\n",
    "        self.current_actions[current_idx] = action\n",
    "\n",
    "        # next_idx==0说明已经对每个智能体进行了一次相应动作的移动，将动作设定到智能体上，并在self.steps大于最大的移动步长时进行截断\n",
    "        # next_idx!=0说明还没对每个智能体设定好动作，将reward值每次清空\n",
    "        if next_idx == 0:\n",
    "            self._execute_world_step()\n",
    "            self.steps += 1\n",
    "            if self.steps >= self.max_cycles:\n",
    "                for a in self.agents:\n",
    "                    self.truncations[a] = True\n",
    "        else:\n",
    "            self._clear_rewards()\n",
    "\n",
    "        # 将累计回报清空并重新进行累计求和\n",
    "        self._cumulative_rewards[cur_agent] = 0\n",
    "        self._accumulate_rewards()\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self.render()\n",
    "\n",
    "    # 创建窗口并启动render\n",
    "    def enable_render(self, mode=\"human\"):\n",
    "        if not self.renderOn and mode == \"human\":\n",
    "            # 创建一个大小与前面设定的screen大小一样的窗口，用来放置之前的scale\n",
    "            self.screen = pygame.display.set_mode(self.screen.get_size())\n",
    "            self.renderOn = True\n",
    "\n",
    "    # 进行智能体的render操作\n",
    "    def render(self):\n",
    "        if self.render_mode is None:\n",
    "            gymnasium.logger.warn(\n",
    "                \"You are calling render method without specifying any render mode.\"\n",
    "            )\n",
    "            return\n",
    "\n",
    "        self.enable_render(self.render_mode)\n",
    "\n",
    "        self.draw()\n",
    "        if self.render_mode == \"rgb_array\":\n",
    "            # 将pygame绘图保存为rgb_array的格式，并逐帧返回\n",
    "            observation = np.array(pygame.surfarray.pixels3d(self.screen))\n",
    "            return np.transpose(observation, axes=(1, 0, 2))\n",
    "        elif self.render_mode == \"human\":\n",
    "            # 将绘图进行刷新显示\n",
    "            pygame.display.flip()\n",
    "            return\n",
    "\n",
    "    # 绘制每个step下的pygame图像\n",
    "    def draw(self):\n",
    "        # 背景设置为白色\n",
    "        self.screen.fill((255, 255, 255))\n",
    "\n",
    "        # 将每个entities的坐标记录，并求出最大的坐标位置\n",
    "        all_poses = [entity.state.p_pos for entity in self.world.entities]\n",
    "        cam_range = np.max(np.abs(np.array(all_poses)))\n",
    "\n",
    "        # update geometry and text positions\n",
    "        text_line = 0\n",
    "        for e, entity in enumerate(self.world.entities):\n",
    "            # 获取坐标，并将y轴翻转，与原本pyglet坐标系相对应\n",
    "            x, y = entity.state.p_pos\n",
    "            y *= (\n",
    "                -1\n",
    "            )  # this makes the display mimic the old pyglet setup (ie. flips image)\n",
    "            \n",
    "            # 将位置坐标进行归一化操作，防止出现过于远离边界\n",
    "            x = (\n",
    "                (x / cam_range) * self.width // 2 * 0.9\n",
    "            )  # the .9 is just to keep entities from appearing \"too\" out-of-bounds\n",
    "            y = (y / cam_range) * self.height // 2 * 0.9\n",
    "            x += self.width // 2\n",
    "            y += self.height // 2\n",
    "\n",
    "            # 绘制圆形以及圆形的边界线\n",
    "            pygame.draw.circle(\n",
    "                self.screen, entity.color * 200, (x, y), entity.size * 350\n",
    "            )  # 350 is an arbitrary scale factor to get pygame to render similar sizes as pyglet\n",
    "            pygame.draw.circle(\n",
    "                self.screen, (0, 0, 0), (x, y), entity.size * 350, 1\n",
    "            )  # borders\n",
    "            assert (\n",
    "                0 < x < self.width and 0 < y < self.height\n",
    "            ), f\"Coordinates {(x, y)} are out of bounds.\"\n",
    "\n",
    "            # 书写对应的文字到 (0,0) 坐标位置\n",
    "            if isinstance(entity, Agent):\n",
    "                if entity.silent:\n",
    "                    continue\n",
    "                if np.all(entity.state.c == 0):\n",
    "                    word = \"_\"\n",
    "                elif self.continuous_actions:\n",
    "                    word = (\n",
    "                        \"[\" + \",\".join([f\"{comm:.2f}\" for comm in entity.state.c]) + \"]\"\n",
    "                    )\n",
    "                else:\n",
    "                    alphabet = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "                    word = alphabet[np.argmax(entity.state.c)]\n",
    "\n",
    "                message = entity.name + \" sends \" + word + \"   \"\n",
    "                message_x_pos = self.width * 0.05\n",
    "                message_y_pos = self.height * 0.95 - (self.height * 0.05 * text_line)\n",
    "                self.game_font.render_to(\n",
    "                    self.screen, (message_x_pos, message_y_pos), message, (0, 0, 0)\n",
    "                )\n",
    "                text_line += 1\n",
    "\n",
    "    # 关闭render相关设定\n",
    "    def close(self):\n",
    "        if self.renderOn:\n",
    "            # 将剩余事件处理\n",
    "            pygame.event.pump()\n",
    "            # 退出pygame展示\n",
    "            pygame.display.quit()\n",
    "            self.renderOn = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(env):\n",
    "    if env.continuous_actions:\n",
    "        env = wrappers.ClipOutOfBoundsWrapper(env)\n",
    "    else:\n",
    "        env = wrappers.AssertOutOfBoundsWrapper(env)\n",
    "    env = wrappers.OrderEnforcingWrapper(env)\n",
    "    return env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser(\"Reinforcement Learning experiments for multiagent environments\")\n",
    "parser.add_argument(\"--num-good\", type=int, default=1, help=\"好agent的个数\")\n",
    "parser.add_argument(\"--num-adversaries\", type=int, default=3, help=\"敌对agent个数\")\n",
    "parser.add_argument(\"--num_obstacles\", type=int, default=2, help=\"障碍物landmark的个数\")\n",
    "parser.add_argument(\"--max_cycles\", type=int, default=25, help=\"每轮最大迭代次数\")\n",
    "parser.add_argument(\"--continuous_actions\", type=bool, default=False, help=\"动作是否采用连续空间\")\n",
    "parser.add_argument(\"--render_mode\", type=str, default='human', help=\"是否进行render\")\n",
    "\n",
    "# 如果在vscode需要解注下列参数\n",
    "parser.add_argument(\"--ip\")\n",
    "parser.add_argument(\"--stdin\")\n",
    "parser.add_argument(\"--control\")\n",
    "parser.add_argument(\"--hb\")\n",
    "parser.add_argument(\"--Session.signature_scheme\")\n",
    "parser.add_argument(\"--Session.key\")\n",
    "parser.add_argument(\"--shell\")\n",
    "parser.add_argument(\"--transport\")\n",
    "parser.add_argument(\"--iopub\")\n",
    "parser.add_argument(\"--f\")\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario = Scenario()\n",
    "world = scenario.make_world(args.num_good,args.num_adversaries, args.num_obstacles)\n",
    "env=SimpleEnv(\n",
    "            scenario=scenario,\n",
    "            world=world,\n",
    "            render_mode=args.render_mode,\n",
    "            max_cycles=args.max_cycles,\n",
    "            continuous_actions=args.continuous_actions,\n",
    "        )\n",
    "env=make_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "for agent in env.agent_iter():\n",
    "    env.render()\n",
    "    observation, reward, termination, truncation, info = env.last()\n",
    "    \n",
    "    if termination or truncation:\n",
    "        action = None\n",
    "    else:\n",
    "        action = env.action_space(agent).sample() # this is where you would insert your policy\n",
    "        \n",
    "    env.step(action) \n",
    "    sleep(0.01)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16, 16, 16]\n",
      "[5, 5, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\pettingzoo\\utils\\wrappers\\base.py:59: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\n",
      "  \"The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\"\n",
      "d:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\pettingzoo\\utils\\wrappers\\base.py:73: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\n",
      "  \"The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\"\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "\n",
    "# env.agents\n",
    "# ['adversary_0', 'adversary_1', 'adversary_2', 'agent_0']\n",
    "# n_player：总的智能体个数\n",
    "# n_agents：adv智能体个数\n",
    "# obs_shape：adv智能体观测空间维度列表\n",
    "# action_shape：adv智能体动作空间维度列表\n",
    "\n",
    "args.n_players = len(env.agents)\n",
    "args.n_agents = args.n_players-1\n",
    "args.obs_shape = []\n",
    "args.action_shape=[]\n",
    "for i in range(args.n_agents):  \n",
    "    cur_agent=env.agents[i]\n",
    "    args.obs_shape.append(env.observation_spaces[cur_agent].shape[0])\n",
    "    args.action_shape.append(env.action_spaces[cur_agent].n)\n",
    "\n",
    "args.high_action = 1\n",
    "args.low_action = -1\n",
    "# print(args.obs_shape)\n",
    "# [16, 16, 16]\n",
    "# print(args.action_shape)\n",
    "# [5, 5, 5]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
