<!--
 * @Author: gongweijing 876887913@qq.com
 * @Date: 2023-12-04 13:33:29
 * @LastEditors: gongweijing 876887913@qq.com
 * @LastEditTime: 2023-12-05 16:16:01
 * @FilePath: /root/Ship_New/readme.md
 * @Description: 这是默认设置,请设置`customMade`, 打开koroFileHeader查看配置 进行设置: https://github.com/OBKoro1/koro1FileHeader/wiki/%E9%85%8D%E7%BD%AE
-->
# Linux下进行项目部署
```
pip install -r requirements.txt
sudo apt-get install libglu1-mesa-dev
python3 shipenv_add_rb.py
```
# 计划的的一些任务
1. 利用老师给的资料进行hybrid action space reinforce learning算法的测试;
2. 对现有的奖励函数进行多样化加权，增加训练算法的可行性;
3. 添加蓝A抓住红A的情况的结束条件;
4. 增加更容易让各个智能体得到训练的初始化条件。

# 当前想做的工作
1. 跑通[hyar](https://zhuanlan.zhihu.com/p/596495689)相关的算法，找出一个可行的算法备选进行动作空间的搜索，并确保算法本身无问题。
2. 训练一些其中附带的算法，例如p_ddpg等算法，保证算法多样性。
3. 理解其中环境的设定，熟悉内部的环境部署算法。

## HyAR项目学习
1. 熟悉项目基本内容,了解项目基本框架。
2. 熟悉环境配置，掌握其环境的基本搭建方法。
3. 熟悉HyAR算法的基本框架，基本部署到现有环境。

## 查阅相关文献确定船只追捕之类的奖励函数设定经验
1. todo

## 了解多智能体调优方案
1. todo

## 待改动bug
1. 红b的速度跟加速度;

## 算法配置
### 纯规则算法
1. 未发现蓝A的时候采用固定的巡航路线进行移动;
2. 

## 环境设定
### 数制转换
1. 0.1km对应的是环境中的数值1;
2. 采用的数值对应的时间单位为/min；
3. 

### 简化部分
1. 红A初始化为地图中心,即初始的坐标值设定为(0,0)，随机选取速度方向，初始速度值根据给定的
1. 红B(诱骗)假设为一直开启诱骗，只要进入蓝A的范围之内，那么就会将其认定为红A，进行追逐。
2. 红b(干扰)假设为一直开启干扰，变为一个0~1的数值，设定最大的干扰范围为:m，干扰的实际距离则为：0~m(m=0.8km),
进入干扰范围后的探测范围公式为:e_dist = e_dist * (1 - 1/d);
3. 蓝A初始化的范围为: 距离其圆心的距离为:[e_{dist}_b/3,2*e_{dist}_b/3]
4. 红B与红A初始距离为:0.5~1km距离的随机位置;红b与红A初始距离为:0.8~1.2km距离的随机位置;
二者的具体位置的话一个相对于运动方向的左方,一个在相对于红A在右边;


### 添加部分
1. 给每个智能体的中心添加一个标识，例如三角、矩形等，加上注释。
2. 智能体的观测空间应当再包含进去其可以观测到的智能体的状态.

## 未来目标
1. 蓝A仍非智能体，后续修改为2个红b和红B两种情况；
2. 蓝A为智能体应当学习到要去追的目标；
3. 
